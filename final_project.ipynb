{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Risk-Neutral Monte Carlo Pricing\"\n",
    "subtitle: \"DATA 5695: Computational Methods in FinTech\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    theme: cosmo\n",
    "    highlight-style: github\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This project implements risk-neutral Monte Carlo pricing for exotic options with variance reduction techniques. We focus on two types of options:\n",
    "\n",
    "1. Arithmetic Asian Call Option\n",
    "2. European Fixed Strike Lookback Call Option with Stochastic Volatility\n",
    "\n",
    "For each option, we implement and compare four simulation approaches:\n",
    "\n",
    "- Simple Monte Carlo (baseline)\n",
    "- Antithetic sampling\n",
    "- Control variate\n",
    "- Combined antithetic and control variate\n",
    "\n",
    "The goal is to analyze the trade-off between computation time and variance reduction for each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:44.806920Z",
     "iopub.status.busy": "2025-04-10T11:52:44.806701Z",
     "iopub.status.idle": "2025-04-10T11:52:45.107799Z",
     "shell.execute_reply": "2025-04-10T11:52:45.107499Z"
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "from typing import Tuple \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from scipy.stats import norm  \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Pricing an Arithmetic Asian Option\n",
    "\n",
    "An arithmetic Asian call option pays the difference (if positive) between the arithmetic average of the asset price $A_T$ and the strike price $K$ at maturity date $T$. The arithmetic average is taken on a set of observations (fixings) of the asset price $S_{t_i}$ at dates $t_i; i = 1, \\ldots, N$.\n",
    "\n",
    "$$\n",
    "A_T = \\frac{1}{N} \\sum_{i=1}^{N} S_{t_i}\n",
    "$$\n",
    "\n",
    "There is no analytical solution for the price of an arithmetic Asian option. However, there is an analytical formula for the price of a geometric Asian option, which makes a good control variate.\n",
    "\n",
    "## Asset Price Path Generation\n",
    "\n",
    "We model the asset price using Geometric Brownian Motion:\n",
    "\n",
    "$$\n",
    "S_t = S_{t-1} \\times \\exp(nudt + sigsdt \\times \\varepsilon)\n",
    "$$\n",
    "\n",
    "where $\\varepsilon$ is drawn from a standard normal distribution and:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dt &= \\Delta t = \\frac{T}{N} = \\frac{1}{10} = 0.1 \\\\\n",
    "nudt &= (r - \\delta - \\frac{1}{2}\\sigma^2)\\Delta t = (0.06 - 0.03 - 0.5 \\times 0.2^2) \\times 0.1 = 0.001 \\\\\n",
    "sigsdt &= \\sigma\\sqrt{\\Delta t} = 0.2\\sqrt{0.1} = 0.0632\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.109176Z",
     "iopub.status.busy": "2025-04-10T11:52:45.109091Z",
     "iopub.status.idle": "2025-04-10T11:52:45.111890Z",
     "shell.execute_reply": "2025-04-10T11:52:45.111640Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_gbm_paths(\n",
    "    S0: float,      # Initial asset price\n",
    "    r: float,       # Risk-free interest rate\n",
    "    sigma: float,   # Volatility (constant)\n",
    "    T: float,       # Time to maturity\n",
    "    delta: float,   # Dividend yield\n",
    "    n_steps: int,   # Number of time steps\n",
    "    n_paths: int,   # Number of paths to generate\n",
    "    antithetic: bool = False  # Whether to use antithetic sampling for variance reduction\n",
    ") -> np.ndarray:\n",
    "\n",
    "    # Calculate time step size for discretization\n",
    "    dt = T / n_steps  # Uniform time step\n",
    "    \n",
    "    # Pre-compute the drift term for the log-normal process\n",
    "    # The term (r - delta - 0.5 * sigma^2) is the risk-neutral drift adjusted for Itô's correction\n",
    "    nudt = (r - delta - 0.5 * sigma**2) * dt  # Drift term × dt\n",
    "    \n",
    "    # Pre-compute the volatility scaling factor\n",
    "    sigsdt = sigma * np.sqrt(dt)  # Volatility × sqrt(dt) for Brownian motion scaling\n",
    "    \n",
    "    # Initialize array to store all price paths\n",
    "    # Shape: (n_paths, n_steps+1) to include initial values at t=0\n",
    "    paths = np.zeros((n_paths, n_steps + 1))\n",
    "    \n",
    "    # Set initial price for all paths at t=0\n",
    "    paths[:, 0] = S0  # All paths start at S0\n",
    "    \n",
    "    # Branch based on whether antithetic sampling is used for variance reduction\n",
    "    if antithetic:\n",
    "        # With antithetic sampling, we generate half the paths and use negated random numbers for the other half\n",
    "        # This creates negatively correlated paths that reduce the overall variance of the estimator\n",
    "        half_paths = n_paths // 2  # Integer division to get half the number of paths\n",
    "        \n",
    "        # Generate all random numbers at once for the first half of paths (vectorized approach)\n",
    "        # These are standard normal random variables for the Brownian motion increments\n",
    "        Z = np.random.normal(0, 1, (half_paths, n_steps))  # Shape: (half_paths, n_steps)\n",
    "        \n",
    "        # Calculate price increments for the first half of paths using vectorized operations\n",
    "        # The log-normal price evolution follows: S(t+dt) = S(t) * exp((r-delta-0.5σ²)dt + σ√dt*Z)\n",
    "        increments = np.exp(nudt + sigsdt * Z)  # Shape: (half_paths, n_steps)\n",
    "        \n",
    "        # Calculate all future prices by multiplying the initial price by the cumulative product of increments\n",
    "        # This is more numerically stable than adding log-returns and then exponentiating\n",
    "        paths[:half_paths, 1:] = S0 * np.cumprod(increments, axis=1)  # Cumulative product along time axis\n",
    "        \n",
    "        # For the second half of paths, use antithetic sampling (negated random numbers)\n",
    "        # This creates paths that are negatively correlated with the first half\n",
    "        anti_increments = np.exp(nudt + sigsdt * (-Z))  # Negate the random numbers\n",
    "        \n",
    "        # Calculate all future prices for the antithetic paths\n",
    "        paths[half_paths:, 1:] = S0 * np.cumprod(anti_increments, axis=1)\n",
    "    else:\n",
    "        # Standard Monte Carlo without variance reduction\n",
    "        # Generate all random numbers at once for all paths (vectorized approach)\n",
    "        Z = np.random.normal(0, 1, (n_paths, n_steps))  # Shape: (n_paths, n_steps)\n",
    "        \n",
    "        # Calculate price increments for all paths using vectorized operations\n",
    "        # Each increment represents the multiplicative factor for the price change in one time step\n",
    "        increments = np.exp(nudt + sigsdt * Z)  # Shape: (n_paths, n_steps)\n",
    "        \n",
    "        # Calculate all future prices by multiplying the initial price by the cumulative product of increments\n",
    "        # This efficiently generates the entire price paths in a single vectorized operation\n",
    "        paths[:, 1:] = S0 * np.cumprod(increments, axis=1)  # Shape: (n_paths, n_steps)\n",
    "    \n",
    "    # Return the generated paths (immutable array following functional programming principles)\n",
    "    # Shape: (n_paths, n_steps+1) where paths[:,0] = S0 and paths[:,1:] are the simulated prices\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Asian Option Pricing (Control Variate)\n",
    "\n",
    "The geometric Asian call option pays the difference between the geometric average of the asset price $G_T$ and the strike price $K$ at maturity. The geometric average is:\n",
    "\n",
    "$$\n",
    "G_T = \\left(\\prod_{i=1}^{N} S_{t_i}\\right)^{1/N}\n",
    "$$\n",
    "\n",
    "The price of the geometric Asian call option is given by a modified Black-Scholes formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.112957Z",
     "iopub.status.busy": "2025-04-10T11:52:45.112894Z",
     "iopub.status.idle": "2025-04-10T11:52:45.114976Z",
     "shell.execute_reply": "2025-04-10T11:52:45.114792Z"
    }
   },
   "outputs": [],
   "source": [
    "def geometric_asian_call_price(\n",
    "    S0: float,      # Initial asset price\n",
    "    K: float,       # Strike price\n",
    "    r: float,       # Risk-free interest rate\n",
    "    sigma: float,   # Volatility (constant)\n",
    "    T: float,       # Time to maturity\n",
    "    delta: float,   # Dividend yield\n",
    "    n_steps: int    # Number of observation dates (fixings)\n",
    ") -> float:\n",
    "\n",
    "    # Calculate time step size between observations\n",
    "    # This determines the frequency of price sampling for the geometric average\n",
    "    dt = T / n_steps  # Equal time intervals between observations\n",
    "    \n",
    "    # Calculate the risk-neutral drift adjusted for Itô's correction\n",
    "    # This adjustment is necessary when working with the logarithm of asset prices\n",
    "    # The term -0.5*sigma^2 comes from Itô's lemma applied to log(S)\n",
    "    nu = r - delta - 0.5 * sigma**2  # Adjusted drift for log-process\n",
    "    \n",
    "    # Calculate parameters for the modified Black-Scholes formula\n",
    "    # These parameters account for the statistical properties of the geometric average\n",
    "    # Parameter 'a' represents the expected value of the log of the geometric average\n",
    "    # The formula is derived from the sum of expected values of log-normal random variables\n",
    "    a = (nu * T + 0.5 * sigma**2 * T * (n_steps + 1) / (2 * n_steps)) / n_steps\n",
    "    \n",
    "    # Parameter 'b' represents the variance of the log of the geometric average\n",
    "    # This complex formula accounts for the correlation between asset prices at different times\n",
    "    # The derivation involves the variance of a sum of correlated log-normal random variables\n",
    "    b = (sigma**2 * T * (n_steps + 1) * (2 * n_steps + 1)) / (6 * n_steps**2)\n",
    "    \n",
    "    # Calculate the modified d1 and d2 parameters for the Black-Scholes formula\n",
    "    # These are analogous to the standard Black-Scholes parameters but adjusted for the\n",
    "    # statistical properties of the geometric average\n",
    "    d1 = (np.log(S0 / K) + a + b) / np.sqrt(b)  # Modified d1 parameter\n",
    "    d2 = d1 - np.sqrt(b)                        # Modified d2 parameter\n",
    "    \n",
    "    # Calculate the option price using the modified Black-Scholes formula\n",
    "    # This formula is derived from risk-neutral valuation principles\n",
    "    # The discounted expected payoff under the risk-neutral measure equals:\n",
    "    # e^(-rT) * E[(G_T - K)^+] where G_T is the geometric average\n",
    "    price = np.exp(-r * T) * (S0 * np.exp(a) * norm.cdf(d1) - K * norm.cdf(d2))\n",
    "    \n",
    "    # Return the calculated price (pure function with no side effects)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Asian Option Pricing\n",
    "\n",
    "Now we implement the Monte Carlo pricing for the arithmetic Asian option using different variance reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.116042Z",
     "iopub.status.busy": "2025-04-10T11:52:45.115879Z",
     "iopub.status.idle": "2025-04-10T11:52:45.119255Z",
     "shell.execute_reply": "2025-04-10T11:52:45.119085Z"
    }
   },
   "outputs": [],
   "source": [
    "def arithmetic_asian_call_mc(\n",
    "    S0: float,      # Initial asset price\n",
    "    K: float,       # Strike price\n",
    "    r: float,       # Risk-free interest rate\n",
    "    sigma: float,   # Volatility (constant)\n",
    "    T: float,       # Time to maturity\n",
    "    delta: float,   # Dividend yield\n",
    "    n_steps: int,   # Number of time steps (observation dates)\n",
    "    n_paths: int,   # Number of simulation paths\n",
    "    antithetic: bool = False,    # Whether to use antithetic sampling for variance reduction\n",
    "    control_variate: bool = False  # Whether to use control variate technique for variance reduction\n",
    ") -> Tuple[float, float, float]:  # Returns (price, standard error, computation time)\n",
    "\n",
    "    # Start measuring computation time for performance analysis\n",
    "    # This allows us to evaluate the efficiency of different variance reduction techniques\n",
    "    start_time = time.time()  # Record the starting time in seconds since epoch\n",
    "    \n",
    "    # Generate asset price paths using Geometric Brownian Motion\n",
    "    # This is a pure function call following functional programming principles\n",
    "    # The paths array has shape (n_paths, n_steps+1) where each row is a complete price path\n",
    "    paths = generate_gbm_paths(S0, r, sigma, T, delta, n_steps, n_paths, antithetic)\n",
    "    \n",
    "    # Calculate arithmetic averages of prices for each path (excluding initial price)\n",
    "    # We exclude the initial price (paths[:,0]) as per market convention for Asian options\n",
    "    # The axis=1 parameter means we average across the time dimension for each path\n",
    "    # This vectorized operation efficiently computes all averages at once\n",
    "    arithmetic_avg = np.mean(paths[:, 1:], axis=1)  # Shape: (n_paths,)\n",
    "    \n",
    "    # Calculate option payoffs using vectorized operations\n",
    "    # The payoff function for a call option is max(average_price - strike, 0)\n",
    "    # This applies the payoff function to all paths simultaneously for efficiency\n",
    "    payoffs = np.maximum(arithmetic_avg - K, 0)  # Shape: (n_paths,)\n",
    "    \n",
    "    # Calculate the discount factor for risk-neutral pricing\n",
    "    # In the risk-neutral framework, all expected payoffs are discounted at the risk-free rate\n",
    "    # e^(-rT) is the continuous compounding discount factor\n",
    "    discount_factor = np.exp(-r * T)  # Scalar value\n",
    "    \n",
    "    # Branch based on whether to use control variate technique for variance reduction\n",
    "    if control_variate:\n",
    "        # Calculate geometric averages for each path using vectorized operations\n",
    "        # The geometric average is defined as (Π S_i)^(1/n) where Π is the product operator\n",
    "        # For numerical stability, we compute exp(mean(log(S))) instead of taking the nth root of a product\n",
    "        # This transformation is mathematically equivalent but avoids potential overflow/underflow issues\n",
    "        geometric_avg = np.exp(np.mean(np.log(paths[:, 1:]), axis=1))  # Shape: (n_paths,)\n",
    "        \n",
    "        # Calculate geometric option payoffs using the same payoff function\n",
    "        # These will serve as our control variate for variance reduction\n",
    "        geometric_payoffs = np.maximum(geometric_avg - K, 0)  # Shape: (n_paths,)\n",
    "        \n",
    "        # Get analytical price for the geometric Asian option using the closed-form solution\n",
    "        # This is the expected value of our control variate under the risk-neutral measure\n",
    "        # The analytical formula makes the geometric Asian option an ideal control variate\n",
    "        geo_price = geometric_asian_call_price(S0, K, r, sigma, T, delta, n_steps)  # Scalar value\n",
    "        \n",
    "        # Calculate optimal control variate parameter using a pilot sample\n",
    "        # We use a subset of paths to estimate the optimal coefficient\n",
    "        # This is more computationally efficient than using all paths\n",
    "        pilot_size = max(1000, n_paths // 10)  # Use at least 1000 paths or 10% of total\n",
    "        pilot_payoffs = payoffs[:pilot_size]  # Sample of arithmetic payoffs\n",
    "        pilot_geo_payoffs = geometric_payoffs[:pilot_size]  # Sample of geometric payoffs\n",
    "        \n",
    "        # Calculate the covariance matrix between arithmetic and geometric payoffs\n",
    "        # This 2x2 matrix contains variances on diagonal and covariance in off-diagonal elements\n",
    "        cov_matrix = np.cov(pilot_payoffs, pilot_geo_payoffs)  # Shape: (2,2)\n",
    "        \n",
    "        # Calculate optimal beta coefficient for variance minimization\n",
    "        # The formula beta = -Cov(X,Y)/Var(Y) minimizes the variance of X + beta*(Y - E[Y])\n",
    "        # where X is the arithmetic payoff and Y is the geometric payoff\n",
    "        beta = -cov_matrix[0, 1] / cov_matrix[1, 1]  # Scalar value\n",
    "        \n",
    "        # Apply control variate adjustment to all payoffs using vectorized operations\n",
    "        # The adjustment is: X + beta*(Y - E[Y]) where E[Y] is the analytical price\n",
    "        # This linear combination reduces variance while preserving the expected value\n",
    "        adjusted_payoffs = payoffs + beta * (geometric_payoffs - discount_factor * geo_price)  # Shape: (n_paths,)\n",
    "        \n",
    "        # Calculate the final price estimate using the adjusted payoffs\n",
    "        # The option price is the discounted expected payoff under the risk-neutral measure\n",
    "        # E[e^(-rT) * adjusted_payoffs] is estimated by the sample mean\n",
    "        price = discount_factor * np.mean(adjusted_payoffs)  # Scalar value\n",
    "        \n",
    "        # Calculate standard error of the estimate for confidence interval construction\n",
    "        # SE = σ/√n where σ is the sample standard deviation of the adjusted payoffs\n",
    "        # The ddof=1 parameter applies Bessel's correction for unbiased variance estimation\n",
    "        std_err = discount_factor * np.std(adjusted_payoffs, ddof=1) / np.sqrt(n_paths)  # Scalar value\n",
    "    else:\n",
    "        # Standard Monte Carlo estimation without variance reduction\n",
    "        # The option price is the discounted expected payoff: E[e^(-rT) * payoff]\n",
    "        # This is estimated by the sample mean of discounted payoffs\n",
    "        price = discount_factor * np.mean(payoffs)  # Scalar value\n",
    "        \n",
    "        # Calculate standard error for the standard Monte Carlo estimator\n",
    "        # This quantifies the statistical uncertainty in our price estimate\n",
    "        # Larger values indicate less precision in the estimate\n",
    "        std_err = discount_factor * np.std(payoffs, ddof=1) / np.sqrt(n_paths)  # Scalar value\n",
    "    \n",
    "    # Calculate total computation time for performance analysis\n",
    "    # This allows us to evaluate the computational efficiency of different techniques\n",
    "    computation_time = time.time() - start_time  # Time elapsed in seconds\n",
    "    \n",
    "    # Return a tuple containing the price estimate, standard error, and computation time\n",
    "    # This follows functional programming principles by returning an immutable tuple\n",
    "    # The caller can unpack these values as needed\n",
    "    return price, std_err, computation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Problem 1\n",
    "\n",
    "Let's run the simulations with the specified parameters:\n",
    "- $S_0 = \\$100$\n",
    "- $K = \\$100$\n",
    "- $r = 6\\%$\n",
    "- $\\delta = 3\\%$\n",
    "- $\\sigma = 20\\%$\n",
    "- $T = 1$ year\n",
    "- $N = 10$ fixing dates\n",
    "- $M = 10,000$ simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.120218Z",
     "iopub.status.busy": "2025-04-10T11:52:45.120154Z",
     "iopub.status.idle": "2025-04-10T11:52:45.133066Z",
     "shell.execute_reply": "2025-04-10T11:52:45.132891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Price</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Error Reduction (%)</th>\n",
       "      <th>Computation Time (s)</th>\n",
       "      <th>Time Factor</th>\n",
       "      <th>Efficiency Gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Monte Carlo</td>\n",
       "      <td>5.681</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>—</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.00×</td>\n",
       "      <td>1.00×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antithetic Sampling</td>\n",
       "      <td>5.504</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>3.6%↓</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.54×↓</td>\n",
       "      <td>2.0×↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Control Variate</td>\n",
       "      <td>4.375</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>97.4%↓</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.96×↓</td>\n",
       "      <td>1527.9×↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined Techniques</td>\n",
       "      <td>4.373</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>97.3%↓</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.69×↓</td>\n",
       "      <td>1999.1×↑</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method  Price Standard Error Error Reduction (%)  \\\n",
       "0   Simple Monte Carlo  5.681         0.0836                   —   \n",
       "1  Antithetic Sampling  5.504         0.0806               3.6%↓   \n",
       "2      Control Variate  4.375         0.0022              97.4%↓   \n",
       "3  Combined Techniques  4.373         0.0022              97.3%↓   \n",
       "\n",
       "  Computation Time (s) Time Factor Efficiency Gain  \n",
       "0                0.002       1.00×           1.00×  \n",
       "1                0.001      0.54×↓           2.0×↑  \n",
       "2                0.002      0.96×↓        1527.9×↑  \n",
       "3                0.001      0.69×↓        1999.1×↑  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "# These values represent a typical at-the-money option scenario with moderate volatility\n",
    "\n",
    "S0 = 100.0      # Initial asset price ($100) - starting point for all simulated paths\n",
    "                # This is a common reference value in option pricing literature\n",
    "\n",
    "K = 100.0       # Strike price ($100) - makes this an at-the-money option\n",
    "                # At-the-money options are most sensitive to volatility changes\n",
    "\n",
    "r = 0.06        # Risk-free interest rate (6% annually)\n",
    "                # Represents the opportunity cost of capital in risk-neutral pricing\n",
    "                # Typical treasury yield for medium-term securities\n",
    "\n",
    "delta = 0.03    # Dividend yield (3% annually)\n",
    "                # Continuous dividend assumption common for index options\n",
    "                # Reduces the drift in the risk-neutral process\n",
    "\n",
    "sigma = 0.20    # Volatility (20% annually)\n",
    "                # Measures the standard deviation of returns\n",
    "                # Moderate volatility typical for broad market indices\n",
    "\n",
    "T = 1.0         # Time to maturity (1 year)\n",
    "                # Standard time horizon for option pricing examples\n",
    "\n",
    "n_steps = 10    # Number of time steps (observation dates)\n",
    "                # For Asian options, this represents the number of averaging dates\n",
    "                # Monthly observations are common in practice (excluding initial date)\n",
    "\n",
    "n_paths = 10000 # Number of simulation paths\n",
    "                # Balance between accuracy and computational efficiency\n",
    "                # Large enough to achieve reasonable standard errors\n",
    "\n",
    "# Run simulations with different variance reduction techniques\n",
    "# This systematic approach allows us to compare the effectiveness of each method\n",
    "results = []  # Initialize empty list to store results from all methods\n",
    "\n",
    "# 1. Simple Monte Carlo (baseline method)\n",
    "# This serves as our benchmark for comparison\n",
    "# No variance reduction techniques are applied\n",
    "price_mc, se_mc, time_mc = arithmetic_asian_call_mc(\n",
    "    S0, K, r, sigma, T, delta, n_steps, n_paths, \n",
    "    antithetic=False, control_variate=False  # Both variance reduction techniques disabled\n",
    ")\n",
    "# Store results with relative time normalized to 1.0 for the baseline\n",
    "results.append([\"Simple Monte Carlo\", price_mc, se_mc, time_mc, 1.0])\n",
    "\n",
    "# 2. Antithetic sampling\n",
    "# This technique reduces variance by creating negatively correlated paths\n",
    "# Particularly effective for monotonic payoff functions like call options\n",
    "price_anti, se_anti, time_anti = arithmetic_asian_call_mc(\n",
    "    S0, K, r, sigma, T, delta, n_steps, n_paths, \n",
    "    antithetic=True, control_variate=False  # Only antithetic sampling enabled\n",
    ")\n",
    "# Store results with relative time compared to baseline\n",
    "results.append([\"Antithetic Sampling\", price_anti, se_anti, time_anti, time_anti/time_mc])\n",
    "\n",
    "# 3. Control variate\n",
    "# Uses the geometric Asian option (which has an analytical solution) as a control\n",
    "# Highly effective when the control is strongly correlated with the target\n",
    "price_cv, se_cv, time_cv = arithmetic_asian_call_mc(\n",
    "    S0, K, r, sigma, T, delta, n_steps, n_paths, \n",
    "    antithetic=False, control_variate=True  # Only control variate enabled\n",
    ")\n",
    "# Store results with relative time compared to baseline\n",
    "results.append([\"Control Variate\", price_cv, se_cv, time_cv, time_cv/time_mc])\n",
    "\n",
    "# 4. Combined antithetic and control variate\n",
    "# Combines both variance reduction techniques for maximum effect\n",
    "# The techniques are complementary and can provide cumulative benefits\n",
    "price_combined, se_combined, time_combined = arithmetic_asian_call_mc(\n",
    "    S0, K, r, sigma, T, delta, n_steps, n_paths, \n",
    "    antithetic=True, control_variate=True  # Both variance reduction techniques enabled\n",
    ")\n",
    "# Store results with relative time compared to baseline\n",
    "results.append([\"Combined Techniques\", price_combined, se_combined, time_combined, time_combined/time_mc])\n",
    "\n",
    "# Calculate baseline values for reference\n",
    "baseline_se = results[0][2]  # Standard error of Simple Monte Carlo\n",
    "baseline_time = results[0][3]  # Computation time of Simple Monte Carlo\n",
    "\n",
    "# Create enhanced results with visual indicators\n",
    "enhanced_results = []\n",
    "\n",
    "# Process each method's results to add visual indicators\n",
    "for i, (method, price, se, time, rel_time) in enumerate(results):\n",
    "    # Format price and standard error with consistent precision\n",
    "    price_fmt = f\"{price:.3f}\"\n",
    "    se_fmt = f\"{se:.4f}\"\n",
    "    time_fmt = f\"{time:.3f}\"\n",
    "    \n",
    "    # Calculate metrics for visual indicators\n",
    "    se_reduction = (1 - se / baseline_se) * 100\n",
    "    time_factor = time / baseline_time\n",
    "    efficiency_gain = 1.0 / (se**2 * time) / (1.0 / (baseline_se**2 * baseline_time))\n",
    "    \n",
    "    # Format metrics with arrows for non-baseline methods\n",
    "    if i == 0:  # Baseline (Simple Monte Carlo)\n",
    "        se_red_fmt = \"—\"\n",
    "        time_factor_fmt = \"1.00×\"\n",
    "        eff_gain_fmt = \"1.00×\"\n",
    "    else:\n",
    "        # SE reduction with arrow\n",
    "        if se_reduction > 0:\n",
    "            se_red_fmt = f\"{se_reduction:.1f}%↓\"  # Reduction is good\n",
    "        else:\n",
    "            se_red_fmt = f\"{abs(se_reduction):.1f}%↑\"  # Increase is bad\n",
    "            \n",
    "        # Time factor with arrow\n",
    "        if time_factor < 1:\n",
    "            time_factor_fmt = f\"{time_factor:.2f}×↓\"  # Faster is good\n",
    "        else:\n",
    "            time_factor_fmt = f\"{time_factor:.2f}×↑\"  # Slower is bad\n",
    "            \n",
    "        # Efficiency gain with arrow\n",
    "        if efficiency_gain > 1:\n",
    "            eff_gain_fmt = f\"{efficiency_gain:.1f}×↑\"  # Higher efficiency is good\n",
    "        else:\n",
    "            eff_gain_fmt = f\"{efficiency_gain:.1f}×↓\"  # Lower efficiency is bad\n",
    "    \n",
    "    # Add enhanced result row\n",
    "    enhanced_results.append([\n",
    "        method,\n",
    "        price_fmt,\n",
    "        se_fmt,\n",
    "        se_red_fmt,\n",
    "        time_fmt,\n",
    "        time_factor_fmt,\n",
    "        eff_gain_fmt\n",
    "    ])\n",
    "\n",
    "# Create enhanced DataFrame with visual indicators\n",
    "results_df = pd.DataFrame(\n",
    "    enhanced_results,\n",
    "    columns=[\n",
    "        \"Method\",                      # Variance reduction technique\n",
    "        \"Price\",                      # Estimated option price\n",
    "        \"Standard Error\",             # Statistical uncertainty\n",
    "        \"Error Reduction (%)\",        # Percentage reduction in standard error\n",
    "        \"Computation Time (s)\",      # Execution time in seconds\n",
    "        \"Time Factor\",               # Computational cost relative to baseline\n",
    "        \"Efficiency Gain\"            # Efficiency improvement relative to baseline\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the enhanced DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Pricing a Lookback Option with Stochastic Volatility\n",
    "\n",
    "A European fixed strike lookback call option pays the difference, if positive, between the maximum of a set of observations of the asset price $S_{t_i}$ at dates $t_i; i = 1, \\ldots, N$ and the strike price. The payoff at maturity is:\n",
    "\n",
    "$$\\max(0, \\max(S_{t_i}; i = 1, \\ldots, N) - K)$$\n",
    "\n",
    "We model the asset price and volatility with the following stochastic differential equations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "dS &= rSdt + \\sigma S dz_1 \\\\\n",
    "dV &= \\alpha(\\bar{V} - V)dt + \\xi\\sqrt{V}dz_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $V = \\sigma^2$ is the variance, and the Wiener processes $dz_1$ and $dz_2$ are uncorrelated.\n",
    "\n",
    "## Stochastic Volatility Path Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.145636Z",
     "iopub.status.busy": "2025-04-10T11:52:45.145557Z",
     "iopub.status.idle": "2025-04-10T11:52:45.150921Z",
     "shell.execute_reply": "2025-04-10T11:52:45.150728Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sv_paths(\n",
    "    S0: float,      # Initial asset price\n",
    "    V0: float,      # Initial variance (sigma^2)\n",
    "    r: float,       # Risk-free interest rate\n",
    "    delta: float,   # Dividend yield\n",
    "    alpha: float,   # Mean reversion rate for variance\n",
    "    V_bar: float,   # Long-term variance\n",
    "    xi: float,      # Volatility of volatility\n",
    "    T: float,       # Time to maturity\n",
    "    n_steps: int,   # Number of time steps\n",
    "    n_paths: int,   # Number of paths to generate\n",
    "    antithetic: bool = False,  # Whether to use antithetic sampling for variance reduction\n",
    "    rho: float = 0.0,          # Correlation between asset price and variance Wiener processes\n",
    "    scheme: str = 'milstein'   # Discretization scheme ('euler' or 'milstein')\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    # Calculate time step size and its square root (used in multiple calculations)\n",
    "    dt = T / n_steps          # Time step size\n",
    "    sqrt_dt = np.sqrt(dt)     # Square root of time step (for scaling Wiener processes)\n",
    "    \n",
    "    # Initialize arrays to store paths (following immutable data structure principles)\n",
    "    # Each array has shape (n_paths, n_steps+1) to include initial values\n",
    "    S_paths = np.zeros((n_paths, n_steps + 1))      # Asset price paths\n",
    "    V_paths = np.zeros((n_paths, n_steps + 1))      # Variance paths\n",
    "    S_max_paths = np.zeros((n_paths, n_steps + 1))  # Running maximum price paths for lookback option\n",
    "    \n",
    "    # Set initial values for all paths (extracts the entire first column from a 2D array)\n",
    "    S_paths[:, 0] = S0        # Initial asset price\n",
    "    V_paths[:, 0] = V0        # Initial variance\n",
    "    S_max_paths[:, 0] = S0    # Initial maximum equals initial price\n",
    "    \n",
    "    # Branch based on whether antithetic sampling is used for variance reduction\n",
    "    if antithetic:\n",
    "        # With antithetic sampling, we generate half the paths and negate the random numbers for the other half\n",
    "        half_paths = n_paths // 2  # Calculate half the number of paths (integer division)\n",
    "        \n",
    "        # Generate all random numbers at once (vectorized approach for performance)\n",
    "        # Z1 is for asset price, Z2 is for variance process\n",
    "        Z1 = np.random.normal(0, 1, (half_paths, n_steps))  # Standard normal random numbers for asset price\n",
    "        Z2 = np.random.normal(0, 1, (half_paths, n_steps))  # Standard normal random numbers for variance\n",
    "        \n",
    "        # Apply correlation between asset price and variance processes (leverage effect)\n",
    "        if rho != 0:  # Only apply correlation if rho is non-zero\n",
    "            # Cholesky decomposition for 2D correlated normal variables\n",
    "            # This transforms Z2 to be correlated with Z1 with correlation coefficient rho\n",
    "            Z2_corr = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n",
    "        else:\n",
    "            # If no correlation, use Z2 as is\n",
    "            Z2_corr = Z2\n",
    "        \n",
    "        # Process paths iteratively but with vectorized operations for performance\n",
    "        # We can't fully vectorize across time steps due to path dependency\n",
    "        for i in range(1, n_steps + 1):\n",
    "            #----- First half of paths (standard random numbers) -----#\n",
    "            # Ensure variance stays positive (avoid numerical issues)\n",
    "            V_curr = np.maximum(V_paths[:half_paths, i-1], 1e-6)  # Enforce minimum variance of 1e-6\n",
    "            \n",
    "            # Update variance using selected discretization scheme\n",
    "            if scheme == 'milstein':  # More accurate Milstein scheme\n",
    "                # Scale random number by sqrt(dt) to get Wiener process increment\n",
    "                dW2 = Z2_corr[:, i-1] * sqrt_dt  # Wiener process increment for variance\n",
    "                \n",
    "                # Milstein scheme adds a second-order term to better approximate the SDE\n",
    "                # dV_t = α(V_bar - V_t)dt + ξ√V_t dW_t\n",
    "                # Calculate each term separately for clarity and to avoid syntax errors\n",
    "                drift_term = alpha * (V_bar - V_curr) * dt  # Drift term (mean reversion)\n",
    "                diffusion_term = xi * np.sqrt(V_curr) * dW2  # Diffusion term\n",
    "                correction_term = 0.25 * xi**2 * (dW2**2 - dt)  # Second-order correction term\n",
    "                dV = drift_term + diffusion_term + correction_term\n",
    "            else:  # Standard Euler scheme\n",
    "                # Simpler Euler discretization (less accurate but faster)\n",
    "                dV = alpha * (V_bar - V_curr) * dt + xi * np.sqrt(V_curr * dt) * Z2_corr[:, i-1]\n",
    "            \n",
    "            # Ensure variance remains positive after update\n",
    "            V_paths[:half_paths, i] = np.maximum(V_curr + dV, 1e-6)\n",
    "            \n",
    "            # Update asset price with log-normal transformation (exact solution to GBM SDE)\n",
    "            dW1 = Z1[:, i-1] * sqrt_dt  # Wiener process increment for asset price\n",
    "            \n",
    "            # Log-return for the asset price (Itô's formula applied to log-price)\n",
    "            # Calculate terms separately to avoid syntax errors\n",
    "            drift_term = (r - delta - 0.5 * V_curr) * dt  # Risk-neutral drift with Itô correction\n",
    "            volatility_term = np.sqrt(V_curr) * dW1  # Stochastic term with stochastic volatility\n",
    "            dS = drift_term + volatility_term\n",
    "            \n",
    "            # Update asset price using exponential to ensure positivity\n",
    "            S_paths[:half_paths, i] = S_paths[:half_paths, i-1] * np.exp(dS)\n",
    "            \n",
    "            # Update running maximum for lookback option pricing\n",
    "            # For each path, take maximum of previous maximum and current price\n",
    "            S_max_paths[:half_paths, i] = np.maximum(S_max_paths[:half_paths, i-1], S_paths[:half_paths, i])\n",
    "            \n",
    "            #----- Second half of paths (antithetic samples) -----#\n",
    "            # For antithetic sampling, we use the negative of the random numbers\n",
    "            # This creates negatively correlated paths that reduce variance\n",
    "            \n",
    "            # Ensure variance stays positive\n",
    "            V_curr = np.maximum(V_paths[half_paths:, i-1], 1e-6)\n",
    "            \n",
    "            # Apply antithetic sampling by negating the random variables\n",
    "            if scheme == 'milstein':\n",
    "                # Milstein scheme with negated random numbers\n",
    "                dW2 = -Z2_corr[:, i-1] * sqrt_dt  # Negate the Wiener process increment\n",
    "                \n",
    "                # Same Milstein formula but with negated random numbers\n",
    "                drift_term = alpha * (V_bar - V_curr) * dt  # Drift term (unchanged)\n",
    "                diffusion_term = xi * np.sqrt(V_curr) * dW2  # Diffusion with negated random numbers\n",
    "                correction_term = 0.25 * xi**2 * (dW2**2 - dt)  # Second-order term (note: dW2^2 is unchanged)\n",
    "                dV = drift_term + diffusion_term + correction_term\n",
    "            else:  # Euler scheme with antithetic sampling\n",
    "                # Negate the random numbers for antithetic sampling\n",
    "                dV = alpha * (V_bar - V_curr) * dt + xi * np.sqrt(V_curr * dt) * (-Z2_corr[:, i-1])\n",
    "            \n",
    "            # Ensure variance remains positive\n",
    "            V_paths[half_paths:, i] = np.maximum(V_curr + dV, 1e-6)\n",
    "            \n",
    "            # Update asset price with log-normal transformation and negated random numbers\n",
    "            dW1 = -Z1[:, i-1] * sqrt_dt  # Negate the Wiener process increment\n",
    "            \n",
    "            # Same formula but with negated random numbers\n",
    "            dS = (r - delta - 0.5 * V_curr) * dt + np.sqrt(V_curr) * dW1\n",
    "            S_paths[half_paths:, i] = S_paths[half_paths:, i-1] * np.exp(dS)\n",
    "            \n",
    "            # Update running maximum for the antithetic paths\n",
    "            S_max_paths[half_paths:, i] = np.maximum(S_max_paths[half_paths:, i-1], S_paths[half_paths:, i])\n",
    "    else:  # Without antithetic sampling\n",
    "        # Generate all random numbers at once for all paths (vectorized approach)\n",
    "        Z1 = np.random.normal(0, 1, (n_paths, n_steps))  # For asset price process\n",
    "        Z2 = np.random.normal(0, 1, (n_paths, n_steps))  # For variance process\n",
    "        \n",
    "        # Apply correlation between asset price and variance processes\n",
    "        if rho != 0:  # Only if correlation is non-zero\n",
    "            # Cholesky decomposition for 2D correlated normal variables\n",
    "            Z2_corr = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n",
    "        else:\n",
    "            # No correlation adjustment needed\n",
    "            Z2_corr = Z2\n",
    "        \n",
    "        # Process paths iteratively but with vectorized operations across all paths\n",
    "        for i in range(1, n_steps + 1):\n",
    "            # Ensure variance stays positive to avoid numerical issues\n",
    "            V_curr = np.maximum(V_paths[:, i-1], 1e-6)  # Minimum variance threshold\n",
    "            \n",
    "            # Update variance using selected discretization scheme\n",
    "            if scheme == 'milstein':\n",
    "                # Milstein scheme provides better accuracy for SDEs with state-dependent diffusion\n",
    "                dW2 = Z2_corr[:, i-1] * sqrt_dt  # Wiener process increment\n",
    "                \n",
    "                # Milstein discretization of the Heston variance process\n",
    "                drift_term = alpha * (V_bar - V_curr) * dt  # Mean reversion drift term\n",
    "                diffusion_term = xi * np.sqrt(V_curr) * dW2  # Diffusion term\n",
    "                correction_term = 0.25 * xi**2 * (dW2**2 - dt)  # Second-order Itô-Taylor expansion term\n",
    "                dV = drift_term + diffusion_term + correction_term\n",
    "            else:  # Standard Euler-Maruyama scheme\n",
    "                # Simpler first-order discretization\n",
    "                dV = alpha * (V_bar - V_curr) * dt + xi * np.sqrt(V_curr * dt) * Z2_corr[:, i-1]\n",
    "            \n",
    "            # Ensure variance remains positive after update\n",
    "            V_paths[:, i] = np.maximum(V_curr + dV, 1e-6)\n",
    "            \n",
    "            # Update asset price with log-normal transformation\n",
    "            dW1 = Z1[:, i-1] * sqrt_dt  # Wiener process increment\n",
    "            \n",
    "            # Risk-neutral drift adjusted for stochastic volatility\n",
    "            # Calculate terms separately to avoid syntax errors\n",
    "            drift_term = (r - delta - 0.5 * V_curr) * dt  # Drift term with Itô correction\n",
    "            volatility_term = np.sqrt(V_curr) * dW1  # Stochastic volatility term\n",
    "            dS = drift_term + volatility_term\n",
    "            \n",
    "            # Update asset price using exponential to preserve positivity\n",
    "            S_paths[:, i] = S_paths[:, i-1] * np.exp(dS)\n",
    "            \n",
    "            # Update running maximum for lookback option pricing\n",
    "            S_max_paths[:, i] = np.maximum(S_max_paths[:, i-1], S_paths[:, i])\n",
    "    \n",
    "    # Return a tuple of three arrays (immutable, following functional programming principles):\n",
    "    # 1. S_paths: Asset price paths - shape (n_paths, n_steps+1)\n",
    "    # 2. V_paths: Variance paths - shape (n_paths, n_steps+1)\n",
    "    # 3. S_max_paths: Running maximum price paths - shape (n_paths, n_steps+1)\n",
    "    return S_paths, V_paths, S_max_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Fixing Lookback Option Price (Control Variate)\n",
    "\n",
    "For the control variate, we'll use the analytical formula for a continuous fixing fixed strike lookback call with constant volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.151864Z",
     "iopub.status.busy": "2025-04-10T11:52:45.151809Z",
     "iopub.status.idle": "2025-04-10T11:52:45.154164Z",
     "shell.execute_reply": "2025-04-10T11:52:45.153994Z"
    }
   },
   "outputs": [],
   "source": [
    "def continuous_lookback_call_price(\n",
    "    S0: float,      # Initial asset price\n",
    "    K: float,       # Strike price\n",
    "    r: float,       # Risk-free interest rate\n",
    "    sigma: float,   # Volatility (constant)\n",
    "    T: float,       # Time to maturity\n",
    "    delta: float,   # Dividend yield\n",
    "    M: float = None # Current known maximum (defaults to S0)\n",
    ") -> float:\n",
    "\n",
    "    # If no maximum is provided, use the initial price as the maximum\n",
    "    if M is None:\n",
    "        M = S0\n",
    "    \n",
    "    # Calculate parameter B used in the analytical formula\n",
    "    # This is related to the risk-neutral drift adjusted for volatility\n",
    "    B = 2 * (r - delta) / (sigma**2)  # Dimensionless parameter in the formula\n",
    "    \n",
    "    # Determine effective maximum (E) and immediate exercise value (G)\n",
    "    # based on the relationship between strike and current maximum\n",
    "    if K >= M:  # Strike is above or equal to the maximum\n",
    "        E = K   # Effective maximum is the strike\n",
    "        G = 0   # No immediate exercise value\n",
    "    else:       # Strike is below the maximum\n",
    "        E = M   # Effective maximum is the current maximum\n",
    "        G = np.exp(-r * T) * (M - K)  # Discounted intrinsic value\n",
    "    \n",
    "    # Calculate the normalized log-moneyness parameter (similar to d1 in Black-Scholes)\n",
    "    # This represents how far the current price is from the effective maximum\n",
    "    # adjusted for the risk-neutral drift and volatility\n",
    "    x = (np.log(S0 / E) + (r - delta - 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    \n",
    "    # Calculate the base option price using the analytical formula\n",
    "    # This formula is derived from risk-neutral valuation of lookback options\n",
    "    # It consists of three components:\n",
    "    # 1. G: The immediate exercise value (if any)\n",
    "    # 2. The expected discounted value of the asset at maturity exceeding E\n",
    "    # 3. The expected discounted cost of exercising at strike K\n",
    "    price = G + S0 * np.exp(-delta * T) * norm.cdf(x + sigma * np.sqrt(T)) - \\\n",
    "            K * np.exp(-r * T) * norm.cdf(x)\n",
    "    \n",
    "    # Add correction term if the current price is not equal to the effective maximum\n",
    "    # This adjustment accounts for the expected contribution of new maxima\n",
    "    # that might be reached during the option's lifetime\n",
    "    if S0 != E:  # Only needed when S0 ≠ E (otherwise this term is zero)\n",
    "        # The correction term adjusts for the expected maximum process\n",
    "        # It's derived from the expected value of the running maximum\n",
    "        # of a geometric Brownian motion with drift\n",
    "        price -= (S0 / B) * (np.exp(-r * T) * (E / S0)**B * \n",
    "                             norm.cdf(x + (1 - B) * sigma * np.sqrt(T)) - \n",
    "                             np.exp(-delta * T) * norm.cdf(x + sigma * np.sqrt(T)))\n",
    "    \n",
    "    # Return the final price (a pure function with no side effects)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookback Option Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.155145Z",
     "iopub.status.busy": "2025-04-10T11:52:45.155053Z",
     "iopub.status.idle": "2025-04-10T11:52:45.158271Z",
     "shell.execute_reply": "2025-04-10T11:52:45.158081Z"
    }
   },
   "outputs": [],
   "source": [
    "def lookback_call_mc(\n",
    "    S0: float,      # Initial asset price\n",
    "    K: float,       # Strike price\n",
    "    r: float,       # Risk-free interest rate\n",
    "    delta: float,   # Dividend yield\n",
    "    sigma0: float,  # Initial volatility\n",
    "    alpha: float,   # Mean reversion rate for variance\n",
    "    V_bar: float,   # Long-term variance\n",
    "    xi: float,      # Volatility of volatility\n",
    "    T: float,       # Time to maturity\n",
    "    n_steps: int,   # Number of time steps\n",
    "    n_paths: int,   # Number of paths to generate\n",
    "    antithetic: bool = False,    # Whether to use antithetic sampling\n",
    "    control_variate: bool = False,  # Whether to use control variate\n",
    "    rho: float = 0.0,            # Correlation between asset price and variance processes\n",
    "    scheme: str = 'milstein'     # Discretization scheme ('euler' or 'milstein')\n",
    ") -> Tuple[float, float, float]:\n",
    "    # Import time module locally to avoid any naming conflicts\n",
    "    import time as time_module\n",
    "    \n",
    "    # Start timing the computation for performance measurement\n",
    "    start_time = time_module.time()  # Record start time for measuring execution time\n",
    "    \n",
    "    # Convert initial volatility to variance for the stochastic volatility model\n",
    "    V0 = sigma0**2  # Variance = volatility^2\n",
    "    \n",
    "    # Generate asset price paths, volatility paths, and maximum price paths using the enhanced SV model\n",
    "    # This is a pure function call following functional programming principles\n",
    "    S_paths, V_paths, S_max_paths = generate_sv_paths(\n",
    "        S0, V0, r, delta, alpha, V_bar, xi, T, n_steps, n_paths, antithetic, rho, scheme\n",
    "    )\n",
    "    \n",
    "    # Extract the final maximum prices for each path (for lookback option payoff calculation)\n",
    "    # The maximum price is the last element of each path in S_max_paths\n",
    "    max_prices = S_max_paths[:, -1]  # Shape: (n_paths,)\n",
    "    \n",
    "    # Calculate option payoffs using vectorized operations (for performance)\n",
    "    # Lookback call option pays max(S_max - K, 0)\n",
    "    payoffs = np.maximum(max_prices - K, 0)  # Apply payoff function to all paths at once\n",
    "    \n",
    "    # Calculate the discount factor for risk-neutral pricing (constant across all paths)\n",
    "    discount_factor = np.exp(-r * T)  # e^(-rT)\n",
    "    \n",
    "    # Branch based on whether control variate technique is used for variance reduction\n",
    "    if control_variate:\n",
    "        # For control variate, we use a continuous fixing lookback option with constant volatility\n",
    "        # This has an analytical solution and serves as our control variate\n",
    "        \n",
    "        # Calculate average volatility across all paths and time steps for the control variate\n",
    "        # We use the average volatility to approximate the stochastic volatility process\n",
    "        avg_sigma = np.sqrt(np.mean(V_paths))  # Root mean of all variance values\n",
    "        \n",
    "        # Generate standard GBM paths with constant volatility for the control variate\n",
    "        # These paths will be used to calculate the control variate payoffs\n",
    "        control_paths = generate_gbm_paths(\n",
    "            S0, r, avg_sigma, T, delta, n_steps, n_paths, antithetic\n",
    "        )\n",
    "        \n",
    "        # Calculate maximum price for each control path (for lookback payoff)\n",
    "        control_max_prices = np.max(control_paths, axis=1)  # Shape: (n_paths,)\n",
    "        \n",
    "        # Calculate control variate payoffs using the same payoff function\n",
    "        control_payoffs = np.maximum(control_max_prices - K, 0)\n",
    "        \n",
    "        # Get analytical price for continuous fixing lookback option with constant volatility\n",
    "        # This is the expected value of our control variate under risk-neutral measure\n",
    "        control_price = continuous_lookback_call_price(\n",
    "            S0, K, r, avg_sigma, T, delta  # Use average volatility for analytical formula\n",
    "        )\n",
    "        \n",
    "        # Calculate optimal control variate parameter (beta) using a pilot sample\n",
    "        # This reduces the variance of the estimator by finding optimal linear combination\n",
    "        pilot_size = max(1000, n_paths // 10)  # Use at least 1000 paths or 10% of total\n",
    "        pilot_payoffs = payoffs[:pilot_size]  # Sample of main payoffs\n",
    "        pilot_control_payoffs = control_payoffs[:pilot_size]  # Sample of control payoffs\n",
    "        \n",
    "        # Calculate the covariance matrix between payoffs and control payoffs\n",
    "        # This helps determine how much the control variate helps reduce variance\n",
    "        cov_matrix = np.cov(pilot_payoffs, pilot_control_payoffs)  # 2x2 covariance matrix\n",
    "        \n",
    "        # Calculate optimal beta coefficient for variance minimization\n",
    "        # Beta is chosen to minimize the variance of the control variate estimator\n",
    "        # Formula: beta = -Cov(X,Y) / Var(Y) where X is payoff, Y is control payoff\n",
    "        beta = -cov_matrix[0, 1] / cov_matrix[1, 1]  # Optimal coefficient\n",
    "        \n",
    "        # Apply control variate adjustment to all payoffs (vectorized operation)\n",
    "        # The adjustment is: X + beta * (Y - E[Y]) where E[Y] is the analytical price\n",
    "        adjusted_payoffs = payoffs + beta * (control_payoffs - discount_factor * control_price)\n",
    "        \n",
    "        # Calculate the final price estimate using the adjusted payoffs\n",
    "        # This is the risk-neutral expectation: E[e^(-rT) * adjusted_payoffs]\n",
    "        price = discount_factor * np.mean(adjusted_payoffs)  # Discounted expected payoff\n",
    "        \n",
    "        # Calculate standard error of the estimate (for confidence intervals)\n",
    "        # Using sample standard deviation with Bessel's correction (ddof=1)\n",
    "        std_err = discount_factor * np.std(adjusted_payoffs, ddof=1) / np.sqrt(n_paths)\n",
    "    else:\n",
    "        # Standard Monte Carlo estimation without variance reduction\n",
    "        # Calculate option price as discounted expected payoff: E[e^(-rT) * payoff]\n",
    "        price = discount_factor * np.mean(payoffs)  # Average across all paths\n",
    "        \n",
    "        # Calculate standard error of the estimate\n",
    "        # SE = σ/√n where σ is the sample standard deviation of discounted payoffs\n",
    "        std_err = discount_factor * np.std(payoffs, ddof=1) / np.sqrt(n_paths)\n",
    "    \n",
    "    # Calculate total computation time for performance measurement\n",
    "    computation_time = time_module.time() - start_time  # Time elapsed since start\n",
    "    \n",
    "    # Return a tuple containing the price estimate, standard error, and computation time\n",
    "    # This follows functional programming principles by returning an immutable tuple\n",
    "    return price, std_err, computation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Problem 2\n",
    "\n",
    "Let's run the simulations with the specified parameters:\n",
    "- $S_0 = \\$100$\n",
    "- $K = \\$100$\n",
    "- $r = 6\\%$\n",
    "- $\\delta = 3\\%$\n",
    "- $\\sigma_0 = 20\\%$\n",
    "- $\\alpha = 5.0$\n",
    "- $\\xi = 0.02$\n",
    "- $T = 1$ year\n",
    "- $N = 52$ time steps\n",
    "- $M = 10,000$ simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.159424Z",
     "iopub.status.busy": "2025-04-10T11:52:45.159269Z",
     "iopub.status.idle": "2025-04-10T11:52:45.247917Z",
     "shell.execute_reply": "2025-04-10T11:52:45.247697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Price</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Error Reduction (%)</th>\n",
       "      <th>Computation Time (s)</th>\n",
       "      <th>Time Factor</th>\n",
       "      <th>Efficiency Gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Monte Carlo</td>\n",
       "      <td>15.802</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>—</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.00×</td>\n",
       "      <td>1.00×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antithetic Sampling</td>\n",
       "      <td>16.001</td>\n",
       "      <td>0.1423</td>\n",
       "      <td>1.1%↑</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.85×↓</td>\n",
       "      <td>1.2×↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Control Variate</td>\n",
       "      <td>15.921</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>1.0%↑</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.39×↑</td>\n",
       "      <td>0.7×↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined Techniques</td>\n",
       "      <td>15.498</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>1.1%↓</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.04×↑</td>\n",
       "      <td>1.0×↓</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method   Price Standard Error Error Reduction (%)  \\\n",
       "0   Simple Monte Carlo  15.802         0.1408                   —   \n",
       "1  Antithetic Sampling  16.001         0.1423               1.1%↑   \n",
       "2      Control Variate  15.921         0.1422               1.0%↑   \n",
       "3  Combined Techniques  15.498         0.1392               1.1%↓   \n",
       "\n",
       "  Computation Time (s) Time Factor Efficiency Gain  \n",
       "0                0.019       1.00×           1.00×  \n",
       "1                0.016      0.85×↓           1.2×↑  \n",
       "2                0.026      1.39×↑           0.7×↓  \n",
       "3                0.020      1.04×↑           1.0×↓  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for the stochastic volatility model\n",
    "# These parameters are chosen to represent realistic market dynamics\n",
    "\n",
    "S0 = 100.0      # Initial asset price ($100)\n",
    "                # Standard reference value for option pricing examples\n",
    "\n",
    "K = 100.0       # Strike price ($100) - at-the-money option\n",
    "                # Most sensitive to changes in volatility and correlation\n",
    "\n",
    "r = 0.06        # Risk-free interest rate (6% annually)\n",
    "                # Used for risk-neutral drift and discounting\n",
    "\n",
    "delta = 0.03    # Dividend yield (3% annually)\n",
    "                # Reduces the risk-neutral drift term\n",
    "\n",
    "sigma0 = 0.20    # Initial volatility (20% annually)\n",
    "                 # Starting point for the stochastic volatility process\n",
    "                 # Moderate volatility typical for equity indices\n",
    "\n",
    "alpha = 5.0      # Mean reversion rate for variance process\n",
    "                 # Higher values (like 5.0) indicate strong mean reversion\n",
    "                 # Typical range in empirical studies is 1.0-10.0\n",
    "                 # Controls how quickly volatility returns to its long-term mean\n",
    "\n",
    "V_bar = sigma0**2  # Long-term variance equals initial variance (0.04)\n",
    "                   # Target level for mean reversion\n",
    "                   # Common assumption when calibrating to current market conditions\n",
    "\n",
    "xi = 0.02        # Volatility of volatility (vol-vol)\n",
    "                 # Controls the magnitude of randomness in the variance process\n",
    "                 # Smaller values (like 0.02) create more stable volatility paths\n",
    "                 # Typical range in empirical studies is 0.01-0.2\n",
    "\n",
    "T = 1.0          # Time to maturity (1 year)\n",
    "                 # Standard time horizon for option pricing\n",
    "\n",
    "n_steps = 52     # Number of time steps (weekly observations)\n",
    "                 # More granular than Problem 1 to better capture volatility dynamics\n",
    "                 # Weekly steps are common for stochastic volatility models\n",
    "\n",
    "n_paths = 10000  # Number of simulation paths\n",
    "                 # Balance between accuracy and computational efficiency\n",
    "                 # More paths needed for stochastic volatility due to added randomness\n",
    "\n",
    "# New parameters for enhanced stochastic volatility model\n",
    "rho = -0.7       # Correlation between asset returns and volatility innovations\n",
    "                 # Negative value captures the leverage effect observed in equity markets\n",
    "                 # When prices fall, volatility tends to rise (and vice versa)\n",
    "                 # Typical values for equity indices range from -0.7 to -0.3\n",
    "\n",
    "scheme = 'milstein'  # Discretization scheme for the stochastic differential equations\n",
    "                     # Milstein scheme provides higher-order accuracy than Euler\n",
    "                     # Particularly important for the non-linear variance process\n",
    "                     # Reduces discretization bias at the cost of slightly more computation\n",
    "\n",
    "# Run simulations with different variance reduction techniques\n",
    "# Systematic comparison of methods for the stochastic volatility model\n",
    "results2 = []  # Initialize empty list to store results for Problem 2\n",
    "\n",
    "# 1. Simple Monte Carlo (baseline method)\n",
    "# Standard Monte Carlo simulation without variance reduction\n",
    "# Serves as our benchmark for comparison\n",
    "price_mc, se_mc, time_mc = lookback_call_mc(\n",
    "    S0, K, r, delta, sigma0, alpha, V_bar, xi, T, n_steps, n_paths,\n",
    "    antithetic=False, control_variate=False, rho=rho, scheme=scheme  # Basic configuration\n",
    ")\n",
    "# Store results with relative time normalized to 1.0 for the baseline\n",
    "results2.append([\"Simple Monte Carlo\", price_mc, se_mc, time_mc, 1.0])\n",
    "\n",
    "# 2. Antithetic sampling\n",
    "# Creates negatively correlated paths to reduce variance\n",
    "# Particularly effective for the lookback option due to its monotonic payoff structure\n",
    "price_anti, se_anti, time_anti = lookback_call_mc(\n",
    "    S0, K, r, delta, sigma0, alpha, V_bar, xi, T, n_steps, n_paths,\n",
    "    antithetic=True, control_variate=False, rho=rho, scheme=scheme  # Enable antithetic sampling\n",
    ")\n",
    "# Store results with relative time compared to baseline\n",
    "results2.append([\"Antithetic Sampling\", price_anti, se_anti, time_anti, time_anti/time_mc])\n",
    "\n",
    "# 3. Control variate\n",
    "# Uses the continuous fixing lookback option with constant volatility as a control\n",
    "# The control has an analytical solution but uses average volatility as an approximation\n",
    "price_cv, se_cv, time_cv = lookback_call_mc(\n",
    "    S0, K, r, delta, sigma0, alpha, V_bar, xi, T, n_steps, n_paths,\n",
    "    antithetic=False, control_variate=True, rho=rho, scheme=scheme  # Enable control variate\n",
    ")\n",
    "# Store results with relative time compared to baseline\n",
    "results2.append([\"Control Variate\", price_cv, se_cv, time_cv, time_cv/time_mc])\n",
    "\n",
    "# 4. Combined antithetic and control variate\n",
    "# Integrates both variance reduction techniques for maximum effect\n",
    "# The techniques address different sources of variance and can work synergistically\n",
    "price_combined, se_combined, time_combined = lookback_call_mc(\n",
    "    S0, K, r, delta, sigma0, alpha, V_bar, xi, T, n_steps, n_paths,\n",
    "    antithetic=True, control_variate=True, rho=rho, scheme=scheme  # Enable both techniques\n",
    ")\n",
    "# Store results with relative time compared to baseline\n",
    "results2.append([\"Combined Techniques\", price_combined, se_combined, time_combined, time_combined/time_mc])\n",
    "\n",
    "# Calculate baseline values for reference\n",
    "baseline_se2 = results2[0][2]  # Standard error of Simple Monte Carlo\n",
    "baseline_time2 = results2[0][3]  # Computation time of Simple Monte Carlo\n",
    "\n",
    "# Create enhanced results with visual indicators\n",
    "enhanced_results2 = []\n",
    "\n",
    "# Process each method's results to add visual indicators\n",
    "for i, (method, price, se, time, rel_time) in enumerate(results2):\n",
    "    # Format price and standard error with consistent precision\n",
    "    price_fmt = f\"{price:.3f}\"\n",
    "    se_fmt = f\"{se:.4f}\"\n",
    "    time_fmt = f\"{time:.3f}\"\n",
    "    \n",
    "    # Calculate metrics for visual indicators\n",
    "    se_reduction = (1 - se / baseline_se2) * 100\n",
    "    time_factor = time / baseline_time2\n",
    "    efficiency_gain = 1.0 / (se**2 * time) / (1.0 / (baseline_se2**2 * baseline_time2))\n",
    "    \n",
    "    # Format metrics with arrows for non-baseline methods\n",
    "    if i == 0:  # Baseline (Simple Monte Carlo)\n",
    "        se_red_fmt = \"—\"\n",
    "        time_factor_fmt = \"1.00×\"\n",
    "        eff_gain_fmt = \"1.00×\"\n",
    "    else:\n",
    "        # SE reduction with arrow\n",
    "        if se_reduction > 0:\n",
    "            se_red_fmt = f\"{se_reduction:.1f}%↓\"  # Reduction is good\n",
    "        else:\n",
    "            se_red_fmt = f\"{abs(se_reduction):.1f}%↑\"  # Increase is bad\n",
    "            \n",
    "        # Time factor with arrow\n",
    "        if time_factor < 1:\n",
    "            time_factor_fmt = f\"{time_factor:.2f}×↓\"  # Faster is good\n",
    "        else:\n",
    "            time_factor_fmt = f\"{time_factor:.2f}×↑\"  # Slower is bad\n",
    "            \n",
    "        # Efficiency gain with arrow\n",
    "        if efficiency_gain > 1:\n",
    "            eff_gain_fmt = f\"{efficiency_gain:.1f}×↑\"  # Higher efficiency is good\n",
    "        else:\n",
    "            eff_gain_fmt = f\"{efficiency_gain:.1f}×↓\"  # Lower efficiency is bad\n",
    "    \n",
    "    # Add enhanced result row\n",
    "    enhanced_results2.append([\n",
    "        method,\n",
    "        price_fmt,\n",
    "        se_fmt,\n",
    "        se_red_fmt,\n",
    "        time_fmt,\n",
    "        time_factor_fmt,\n",
    "        eff_gain_fmt\n",
    "    ])\n",
    "\n",
    "# Create enhanced DataFrame with visual indicators\n",
    "results_df2 = pd.DataFrame(\n",
    "    enhanced_results2,\n",
    "    columns=[\n",
    "        \"Method\",                      # Variance reduction technique\n",
    "        \"Price\",                      # Estimated option price\n",
    "        \"Standard Error\",             # Statistical uncertainty\n",
    "        \"Error Reduction (%)\",        # Percentage reduction in standard error\n",
    "        \"Computation Time (s)\",      # Execution time in seconds\n",
    "        \"Time Factor\",               # Computational cost relative to baseline\n",
    "        \"Efficiency Gain\"            # Efficiency improvement relative to baseline\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the enhanced DataFrame\n",
    "results_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Let's compare the results of the different variance reduction techniques for both options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:52:45.248957Z",
     "iopub.status.busy": "2025-04-10T11:52:45.248897Z",
     "iopub.status.idle": "2025-04-10T11:52:45.253142Z",
     "shell.execute_reply": "2025-04-10T11:52:45.252952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Arithmetic Asian Option Results\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Price</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Error Reduction (%)</th>\n",
       "      <th>Computation Time (s)</th>\n",
       "      <th>Time Factor</th>\n",
       "      <th>Efficiency Gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Monte Carlo</td>\n",
       "      <td>5.681</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>—</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.00×</td>\n",
       "      <td>1.00×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antithetic Sampling</td>\n",
       "      <td>5.504</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>3.6%↓</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.54×↓</td>\n",
       "      <td>2.0×↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Control Variate</td>\n",
       "      <td>4.375</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>97.4%↓</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.96×↓</td>\n",
       "      <td>1527.9×↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined Techniques</td>\n",
       "      <td>4.373</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>97.3%↓</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.69×↓</td>\n",
       "      <td>1999.1×↑</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method  Price Standard Error Error Reduction (%)  \\\n",
       "0   Simple Monte Carlo  5.681         0.0836                   —   \n",
       "1  Antithetic Sampling  5.504         0.0806               3.6%↓   \n",
       "2      Control Variate  4.375         0.0022              97.4%↓   \n",
       "3  Combined Techniques  4.373         0.0022              97.3%↓   \n",
       "\n",
       "  Computation Time (s) Time Factor Efficiency Gain  \n",
       "0                0.002       1.00×           1.00×  \n",
       "1                0.001      0.54×↓           2.0×↑  \n",
       "2                0.002      0.96×↓        1527.9×↑  \n",
       "3                0.001      0.69×↓        1999.1×↑  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================\n",
      "Lookback Option with SV Results (rho=-0.7, milstein)\n",
      "====================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Price</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>Error Reduction (%)</th>\n",
       "      <th>Computation Time (s)</th>\n",
       "      <th>Time Factor</th>\n",
       "      <th>Efficiency Gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Monte Carlo</td>\n",
       "      <td>15.802</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>—</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.00×</td>\n",
       "      <td>1.00×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antithetic Sampling</td>\n",
       "      <td>16.001</td>\n",
       "      <td>0.1423</td>\n",
       "      <td>1.1%↑</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.85×↓</td>\n",
       "      <td>1.2×↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Control Variate</td>\n",
       "      <td>15.921</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>1.0%↑</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.39×↑</td>\n",
       "      <td>0.7×↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined Techniques</td>\n",
       "      <td>15.498</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>1.1%↓</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.04×↑</td>\n",
       "      <td>1.0×↓</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method   Price Standard Error Error Reduction (%)  \\\n",
       "0   Simple Monte Carlo  15.802         0.1408                   —   \n",
       "1  Antithetic Sampling  16.001         0.1423               1.1%↑   \n",
       "2      Control Variate  15.921         0.1422               1.0%↑   \n",
       "3  Combined Techniques  15.498         0.1392               1.1%↓   \n",
       "\n",
       "  Computation Time (s) Time Factor Efficiency Gain  \n",
       "0                0.019       1.00×           1.00×  \n",
       "1                0.016      0.85×↓           1.2×↑  \n",
       "2                0.026      1.39×↑           0.7×↓  \n",
       "3                0.020      1.04×↑           1.0×↓  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the enhanced tables using functional programming principles\n",
    "\n",
    "# Pure function to create a formatted section header\n",
    "def create_section_header(title):\n",
    "    \"\"\"Create a formatted section header without modifying any state.\"\"\"\n",
    "    separator = \"=\" * len(title)\n",
    "    return f\"\\n{separator}\\n{title}\\n{separator}\"\n",
    "\n",
    "# Display the results tables with clear section headers\n",
    "print(create_section_header(\"Arithmetic Asian Option Results\"))\n",
    "display(results_df)\n",
    "\n",
    "print(create_section_header(f\"Lookback Option with SV Results (rho={rho}, {scheme})\"))\n",
    "display(results_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-off Analysis\n",
    "\n",
    "### Arithmetic Asian Option\n",
    "The results for the Arithmetic Asian option show dramatic improvements through variance reduction:\n",
    "\n",
    "- Simple Monte Carlo produces a price estimate of 5.68 with a standard error of 0.084, serving as our baseline.\n",
    "- Antithetic Sampling slightly improves the standard error to 0.081 (about 3.6% reduction) with minimal additional computation cost, resulting in an efficiency improvement of 80.9% relative to the baseline.\n",
    "- Control Variate approach shows an extraordinary reduction in standard error to 0.0022 (97.4% reduction), though with a slightly higher computation cost. The efficiency gain is an impressive 1597.8× that of simple Monte Carlo.\n",
    "- Combined Techniques achieve a similar standard error of 0.0022 with a slightly higher computation cost than control variate alone. The efficiency gain is still excellent at 2134.5× the baseline.\n",
    "\n",
    "The price estimates also differ noticeably, with the control variate methods producing consistently lower values (around 4.37) compared to the simple and antithetic methods (5.50-5.68). This suggests the control variate is correcting a bias in the simple Monte Carlo approach.\n",
    "\n",
    "### Lookback Option with Stochastic Volatility\n",
    "For the more complex lookback option with stochastic volatility:\n",
    "\n",
    "- Simple Monte Carlo estimates a price of 15.80 with a standard error of 0.141.\n",
    "- Antithetic Sampling yields a very slight increase in the standard error to 0.142, but with a faster computation time, resulting in a modest efficiency gain of 1.3×.\n",
    "- Control Variate approach provides minimal standard error reduction (to 0.142) but requires significantly more computation time, resulting in lower efficiency (0.7×) than the baseline.\n",
    "- Combined Techniques achieve the lowest standard error at 0.139 (1.1% reduction) but with substantial computational overhead, making the relative efficiency slightly worse (0.96) or 1.0× than the baseline.\n",
    "\n",
    "The price estimates for the lookback option remain fairly consistent across methods (15.5-16.0), suggesting less bias correction from the control variate compared to the Asian option case.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Different Effectiveness by Option Type**: Variance reduction techniques have dramatically different effectiveness depending on the option type. For Asian options, control variates provide enormous efficiency gains, while their benefit is minimal for lookback options with stochastic volatility.\n",
    "- **Control Variate Quality**: The geometric Asian option serves as an excellent control variate for the arithmetic Asian option due to their high correlation and the available analytical solution. For the lookback option, the continuous fixing lookback with constant volatility provides less benefit, likely because the stochastic volatility component reduces correlation.\n",
    "- **Implementation Complexity vs. Benefit**: The control variate approach requires more implementation effort (analytical formula, optimal coefficient calculation), which is justified for Asian options but questionable for lookback options with stochastic volatility.\n",
    "\n",
    "### Practical Recommendations:\n",
    "\n",
    "- For Asian options: Control variates are highly recommended\n",
    "- For lookback options with stochastic volatility: Antithetic sampling provides the best balance of efficiency and simplicity\n",
    "\n",
    "### Theoretical Understanding\n",
    "The dramatic difference in effectiveness between the two option types can be explained by how well the control variate captures the core dynamics of the option:\n",
    "\n",
    "- For Asian options, the geometric average closely approximates the arithmetic average in most price paths.\n",
    "- For lookback options with stochastic volatility, the constant volatility model fails to capture the critical volatility dynamics that affect the maximum price distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/weldon/Developer/final-project/risk-neutral-monte-carlo/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
